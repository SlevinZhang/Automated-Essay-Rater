{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\siliang\n",
      "[nltk_data]     zhang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\siliang\n",
      "[nltk_data]     zhang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\siliang zhang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "import re\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  Dear local newspaper, I think effects computer...   \n",
      "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "   rater1_domain1  rater2_domain1  domain1_score  \n",
      "0             4.0             4.0            8.0  \n",
      "1             5.0             4.0            9.0  \n",
      "2             4.0             3.0            7.0  \n",
      "3             5.0             5.0           10.0  \n",
      "4             4.0             4.0            8.0  \n",
      "total number of essay: 12978\n",
      "=========================================\n",
      "Dear local newspaper\n"
     ]
    }
   ],
   "source": [
    "dataset_root = './dataset/'\n",
    "filename = 'globalenglish_essay_scoring.csv'\n",
    "\n",
    "df = pd.read_csv(dataset_root + filename,encoding='latin-1')\n",
    "print(\"=========================================\")\n",
    "print(df.head(5))\n",
    "\n",
    "print(\"total number of essay: {}\".format(len(df.index)))\n",
    "\n",
    "sample_essay = ''\n",
    "for index in range(df.shape[0]):\n",
    "    sample_essay += df['essay'][index]\n",
    "print(\"=========================================\")\n",
    "print(sample_essay[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dear', 'local', 'newspaper', 'think', 'effects', 'computers', 'people', 'great', 'learning', 'skills/affects', 'give', 'us', 'time', 'chat', 'friends/new', 'people', 'helps', 'us', 'learn', 'globe']\n",
      "1404175\n",
      "['people', 'would', 'caps1', 'computers', 'like', 'one', 'time', 'computer', 'get', 'could', 'books', 'also', 'think', 'building', 'things', 'many', 'go', 'caps2', 'family', 'book', 'even', 'way', 'author', 'parents', 'know', 'life', 'make', 'friends', 'good', 'going', 'offensive', 'want', 'take', 'story', 'us', 'caps3', 'see', 'read', 'new', 'day', 'something', 'home', 'said', 'much', 'got', 'mood', 'library', 'back', 'state', 'dirigibles', 'use', 'num1', 'children', 'help', 'another', 'music', 'cyclist', 'find', 'person1', 'always', 'need', 'really', 'empire', 'around', 'thing', 'say', 'kids', 'caps4', 'person', 'world', 'first', 'right', 'bad', 'made', 'movies', 'libraries', 'someone', 'reason', 'month1', 'paragraph', 'mast', 'never', 'everyone', 'school', 'learn', 'away', 'water', 'different', 'talk', 'patient', 'went', 'believe', 'example', 'organization1', 'great', 'lot', 'love', 'test', 'location1', 'builders']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_tokens = word_tokenize(sample_essay)\n",
    "\n",
    "#remove punctuation\n",
    "nonPunct = re.compile('.*[A-Za-z0-9].*')\n",
    "\n",
    "word_tokens = [w for w in word_tokens if nonPunct.match(w)]\n",
    "\n",
    "#lowercase all words\n",
    "word_tokens = [w.lower() for w in word_tokens]\n",
    "\n",
    "custom_stop_words = set([\"''\",\"``\",\"'s\",\"n't\"])\n",
    "\n",
    "stop_words = set(stopwords.words('english')) | custom_stop_words\n",
    "filtered_words = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "print(filtered_words[:20])\n",
    "print(len(filtered_words))\n",
    "\n",
    "#count the frequency of words\n",
    "fdist = nltk.FreqDist(filtered_words)\n",
    "\n",
    "most_frequent_words = []\n",
    "for word, freq in fdist.most_common(100):\n",
    "    most_frequent_words.append(word)\n",
    "    \n",
    "print(most_frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 55), ('IN', 53), ('PRP', 29), ('VB', 21), ('RB', 21), ('JJ', 20), ('DT', 20), (',', 18), ('NNS', 17), ('.', 16), ('VBP', 14), ('VBZ', 14), ('CC', 14), ('VBG', 14), ('NNP', 12), ('PRP$', 12), ('TO', 10), ('MD', 5), ('WRB', 4), ('VBN', 4), ('RP', 3), (':', 2), ('JJR', 2), ('(', 1), (')', 1), ('EX', 1), ('VBD', 1), ('WP', 1), ('POS', 1)]\n",
      "{'NN': 84, 'VB': 68, 'JJ': 22}\n"
     ]
    }
   ],
   "source": [
    "def pos_tagging_feature(essay):\n",
    "    \"\"\"\n",
    "        we only need \n",
    "        'NN/NNS/NNP/NNPS' -> noun\n",
    "        'VB/VBD/VBG/VBN/VBP/VBZ' -> verb\n",
    "        'JJ/JJR/JJS' -> adjective\n",
    "    \"\"\"\n",
    "    word_tokens = word_tokenize(essay)\n",
    "    words_tagged = nltk.pos_tag(word_tokens)\n",
    "    \n",
    "    tag_freq = nltk.FreqDist([tag for (word, tag) in words_tagged])\n",
    "    \n",
    "    tag_list = tag_freq.most_common()\n",
    "    print(tag_list)\n",
    "    tag_dict = {}\n",
    "    tag_dict['NN'] = sum([ pair[1] for pair in tag_list if re.match(r\"NN.*\",pair[0])])\n",
    "    tag_dict['VB'] = sum([pair[1] for pair in tag_list if re.match(r\"VB.*\",pair[0])])\n",
    "    tag_dict['JJ'] = sum([pair[1] for pair in tag_list if re.match(r\"JJ.*\",pair[0])])\n",
    "    \n",
    "    return tag_dict\n",
    "\n",
    "one_essay = df['essay'][0]\n",
    "tag_dict = pos_tagging_feature(one_essay)\n",
    "print(tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_words': 166, 'num_sentences': 16, 'average_sent_length': 21.875}\n"
     ]
    }
   ],
   "source": [
    "def statistical_feature(essay):\n",
    "    \n",
    "    #average sentence length\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sent_list = sent_detector.tokenize(essay.strip())\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    #sentence count\n",
    "    num_sentence = len(sent_list)\n",
    "\n",
    "    #average length\n",
    "    average_sent_length = sum([len(tokenizer.tokenize(sentence)) for sentence in sent_list ]) / num_sentence\n",
    "    \n",
    "    \n",
    "    word_tokens = tokenizer.tokenize(essay)\n",
    "    \n",
    "    #lower case\n",
    "    word_tokens = [w.lower() for w in word_tokens]\n",
    "    \n",
    "    #remove stopwords\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    word_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "    \n",
    "    #words count\n",
    "    num_words = len(word_tokens)\n",
    "    \n",
    "    statis_dict = {}\n",
    "    statis_dict['num_words'] = num_words\n",
    "    statis_dict['num_sentences'] = num_sentence\n",
    "    statis_dict['average_sent_length'] = average_sent_length\n",
    "\n",
    "    return statis_dict\n",
    "stat = statistical_feature(one_essay)\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def orth_feature(essay):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
